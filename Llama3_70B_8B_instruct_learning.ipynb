{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InovationProject/llm_customization/blob/main/Llama3_70B_8B_instruct_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49QqEw-j9TfD"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6JkFjRr91sA"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "import torch\n",
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "\n",
        "\"\"\"\n",
        "A simple example on using SFTTrainer and Accelerate to finetune Phi-3 models. For\n",
        "a more advanced example, please follow HF alignment-handbook/scripts/run_sft.py.\n",
        "This example has utilized DeepSpeed ZeRO3 offload to reduce the memory usage. The\n",
        "script can be run on V100 or later generation GPUs. Here are some suggestions on\n",
        "futher reducing memory consumption:\n",
        "    - reduce batch size\n",
        "    - decrease lora dimension\n",
        "    - restrict lora target modules\n",
        "Please follow these steps to run the script:\n",
        "1. Install dependencies:\n",
        "    conda install -c conda-forge accelerate\n",
        "    pip3 install -i https://pypi.org/simple/ bitsandbytes\n",
        "    pip3 install peft\n",
        "    pip3 install deepspeed\n",
        "2. Setup accelerate and deepspeed config based on the machine used:\n",
        "    accelerate config\n",
        "Here is a sample config for deepspeed zero3:\n",
        "    compute_environment: LOCAL_MACHINE\n",
        "    debug: false\n",
        "    deepspeed_config:\n",
        "    gradient_accumulation_steps: 1\n",
        "    offload_optimizer_device: none\n",
        "    offload_param_device: none\n",
        "    zero3_init_flag: true\n",
        "    zero3_save_16bit_model: true\n",
        "    zero_stage: 3\n",
        "    distributed_type: DEEPSPEED\n",
        "    downcast_bf16: 'no'\n",
        "    enable_cpu_affinity: false\n",
        "    machine_rank: 0\n",
        "    main_training_function: main\n",
        "    mixed_precision: bf16\n",
        "    num_machines: 1\n",
        "    num_processes: 4\n",
        "    rdzv_backend: static\n",
        "    same_network: true\n",
        "    tpu_env: []\n",
        "    tpu_use_cluster: false\n",
        "    tpu_use_sudo: false\n",
        "    use_cpu: false\n",
        "3. check accelerate config:\n",
        "    accelerate env\n",
        "4. Run the code:\n",
        "    accelerate launch sample_finetune.py\n",
        "\"\"\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "###################\n",
        "# Hyper-parameters\n",
        "###################\n",
        "training_config = {\n",
        "    \"bf16\": True,\n",
        "    \"do_eval\": False,\n",
        "    \"learning_rate\": 5.0e-06,\n",
        "    \"log_level\": \"info\",\n",
        "    \"logging_steps\": 20,\n",
        "    \"logging_strategy\": \"steps\",\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"max_steps\": -1,\n",
        "    \"output_dir\": \"./checkpoint_dir\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"per_device_eval_batch_size\": 1,\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"remove_unused_columns\": True,\n",
        "    \"save_steps\": 20,\n",
        "    \"save_total_limit\": 1,\n",
        "    \"seed\": 0,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"warmup_ratio\": 0.2,\n",
        "    }\n",
        "\n",
        "peft_config = {\n",
        "    \"r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"bias\": \"none\",\n",
        "    \"task_type\": \"CAUSAL_LM\",\n",
        "    \"target_modules\": \"all-linear\",\n",
        "    \"modules_to_save\": None,\n",
        "}\n",
        "train_conf = TrainingArguments(**training_config)\n",
        "peft_conf = LoraConfig(**peft_config)\n",
        "\n",
        "\n",
        "###############\n",
        "# Setup logging\n",
        "###############\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")\n",
        "log_level = train_conf.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "# Log on each process a small summary\n",
        "logger.warning(\n",
        "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
        "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
        ")\n",
        "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
        "logger.info(f\"PEFT parameters {peft_conf}\")\n",
        "\n",
        "\n",
        "################\n",
        "# Modle Loading\n",
        "################\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"   # @param ['meta-llama/Meta-Llama-3-8B-Instruct', 'microsoft/Phi-3-mini-4k-instruct']\n",
        "model_kwargs = dict(\n",
        "    use_cache=False,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.model_max_length = 2048\n",
        "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
        "\n",
        "##################\n",
        "# Data Processing\n",
        "##################\n",
        "def apply_chat_template(\n",
        "    example,\n",
        "    tokenizer,\n",
        "):\n",
        "    messages = example[\"messages\"]\n",
        "    # Add an empty system message if there is none\n",
        "    if messages[0][\"role\"] != \"system\":\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=False)\n",
        "    return example\n",
        "\n",
        "learning_data = \"HuggingFaceH4/ultrachat_200k\"  # @param {type: \"string\"}\n",
        "\n",
        "raw_dataset = load_dataset(learning_data)\n",
        "train_dataset = raw_dataset[\"train_sft\"]\n",
        "test_dataset = raw_dataset[\"test_sft\"]\n",
        "column_names = list(train_dataset.features)\n",
        "\n",
        "learning_data_num = 2000  # @param {type: \"number\"}\n",
        "\n",
        "processed_train_dataset = train_dataset.select(range(learning_data_num)).map(\n",
        "    apply_chat_template,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    num_proc=10,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Applying chat template to train_sft\",\n",
        ")\n",
        "\n",
        "processed_test_dataset = test_dataset.select(range(learning_data_num)).map(\n",
        "    apply_chat_template,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    num_proc=10,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Applying chat template to test_sft\",\n",
        ")\n",
        "\n",
        "\n",
        "###########\n",
        "# Training\n",
        "###########\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=train_conf,\n",
        "    peft_config=peft_conf,\n",
        "    train_dataset=processed_train_dataset,\n",
        "    eval_dataset=processed_test_dataset,\n",
        "    max_seq_length=2048,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True\n",
        ")\n",
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "\n",
        "#############\n",
        "# Evaluation\n",
        "#############\n",
        "tokenizer.padding_side = 'left'\n",
        "metrics = trainer.evaluate()\n",
        "metrics[\"eval_samples\"] = len(processed_test_dataset)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "\n",
        "# ############\n",
        "# # Save model\n",
        "# ############\n",
        "trainer.save_model(train_conf.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUeTJ9GX-30v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "checkpoint = '/content/checkpoint_dir/checkpoint-1140'  # @param {type: \"string\"}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Which famous landmarks should I visit in London, beyond the usual ones?'  # @param {type: \"string\"}\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "s227ibWatWhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olP2cERzbSG8"
      },
      "outputs": [],
      "source": [
        "# ダウンロードしたいフォルダを zip 圧縮する\n",
        "!zip -r /content/checkpoint.zip /content/checkpoint_dir\n",
        "\n",
        "# 圧縮した zip ファイルをダウンロードする\n",
        "from google.colab import files\n",
        "files.download(\"/content/checkpoint.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fsnu6pBl4NJ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNbkN9ZjEttsvkhITXcCY02",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}